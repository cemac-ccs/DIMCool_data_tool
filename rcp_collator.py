#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
iFEED Data consolidation tool v1

Name: rcp_collator.py

Description:
    Combine all data for a single rcp in the iFEED project. This data is comprised of
    120 files per year (10 production levels, 10 irrigation levels), over 100 years.
    Data is combined from 12000 individual ascii files (with 49 columns per file and each
    row relating to a single 0.5degx0.5deg gridcell) into a single NetCDF file using
    iris cubes. Program is set up to use multiprocessing also as the data combination
    process can be time consuming.

Arguments:
    dir   : Location of the folder containing the yearly raw GLAM outputs. This should
            be a directory and contain a set of 120 folders with years from 1980 to 2099.
            The contents of the folder are checked to ensure all years are present.
            The folder itself should have the hierarchy country/crop/model/rcp/years;
            since this hierarchy is determined by outside scripts, it's validity is
            assumed and not checked. Default is the location of this python script
    out   : The directory of the output netCDF file. Default is the location of this python
            script. The output filename is autogenerated from the directory structure as
            $out/$country_$crop_$model_$rcp.nc.
    proc  : Number of parallel processes (maximum 40). Default is 1

Restrictions:
    There should be a set of 120 folders with years from 1980 to 2099 in execution folder
    Assumption that folder structure is country/crop/model/rcp/years

Created December 2019

@author: Christopher Symonds, CEMAC, University of Leeds
"""

import iris
import numpy as np
import pandas as pd
import os
import argparse
import time
from multiprocessing import Pool
import errlib

prod_lst=["0.1","0.2","0.3","0.4","0.5","0.6","0.7","0.8","0.9","1"]
irr_lst=["0","0.1","0.2","0.3","0.4","0.5","0.6","0.7","0.8","0.9","1","2"]

column={'V1': 'Year',
        'V2': 'Latitude',
        'V3': 'Longitude',
        'V4': 'Planting date',
        'V5': 'Final crop stage (ISTG)',
        'V6': 'Mean root length density by volume',
        'V7': 'LAI (specifically RLAI (2))',
        'V8': 'Yield',
        'V9': 'Biomass',
        'V10': 'Empty (irrigated fraction; SLA)',
        'V11': 'Harvest index',
        'V12': 'Cumulative rain',
        'V13': 'Solar radiation',
        'V14': 'Total soil water',
        'V15': 'Transpiration',
        'V16': 'Evapotranspiration_1',
        'V17': 'Potential evapotranspiration (limited by soil transport, LAI and energy)',
        'V18': 'Soil water stress factor',
        'V19': 'Evapotranspiration_2',
        'V20': 'Runoff',
        'V21': 'Cumulative runoff',
        'V22': 'Potential (root-limited) uptake',
        'V23': 'Cumulative potential uptake',
        'V24': 'Drainage',
        'V25': 'Cumulative drainage',
        'V26': 'Potential (energy-limited) transpiration',
        'V27': 'Cumulative potential transpiration',
        'V28': 'Cumulative evaporation',
        'V29': 'Max LAI during growing season',
        'V30': 'Cumulative potential evaporation',
        'V31': 'Cumulative transpiration',
        'V32': 'Root length density by area',
        'V33': 'Root Length Density by Area / LAI',
        'V34': 'Rainfall',
        'V35': 'Change in soil moisture',
        'V36': 'Absorbed radiation',
        'V37': 'Duration',
        'V38': 'Mean vapour pressure deficit',
        'V39': 'Tot net radiation',
        'V40': 'Total percentage of pods setting (TOTPP)',
        'V41': 'Total percentage of pods setting considering temperature only (TOTPP_HIT)',
        'V42': 'Total percentage of pods setting considering water stress only (TOTPP_WAT)',
        'V43': 'Mean temperature during the crop season (planting to harvest).',
        'V44': 'Factor DHDT is reduced by due to heat stress when HTS=1 or 2 (HT_FAC)',
        'V45': 'TOTWHARVDEP',
        'V46': 'STORED_WATER',
        'V47': 'Panicle initiation date (DOY - Sorghum only)',
        'V48': 'Flowering date (DOY - Sorghum only)',
        'V49': 'Total supplementary irrigation added to VOLSW (1) if using SUP irrigation'
        }

var_nm={'V1' : 'year',
        'V2' : 'latitude',
        'V3' : 'longitude',
        'V4' : 'plant_date',
        'V5' : 'istg_final',
        'V6' : 'rlv_mean',
        'V7' : 'rlai_2',
        'V8' : 'yield',
        'V9' : 'biomass',
        'V10': 'sla',
        'V11': 'harv_index',
        'V12': 'tot_rain',
        'V13': 'srad_final',
        'V14': 'soil_wat',
        'V15': 'trans',
        'V16': 'evtrans1',
        'V17': 'pot_evtrans',
        'V18': 'soil_wat_fac',
        'V19': 'evtrans2',
        'V20': 'runoff',
        'V21': 'tot_runoff',
        'V22': 'pot_uptake',
        'V23': 'tot_pot_uptake',
        'V24': 'drainage',
        'V25': 'tot_drainage',
        'V26': 'pot_trans',
        'V27': 'tot_pot_trans',
        'V28': 'tot_evap',
        'V29': 'lai_max',
        'V30': 'tot_pot_ev',
        'V31': 'tot_trans',
        'V32': 'rla',
        'V33': 'rla_over_lai',
        'V34': 'rain_final',
        'V35': 'd_soil_moist',
        'V36': 't_rad_abs',
        'V37': 'dur',
        'V38': 'mean_vap_pres_def',
        'V39': 'Tot_net_rad',
        'V40': 'tot_per_pod',
        'V41': 'tot_per_pod_hit',
        'V42': 'tot_per_pod_wat',
        'V43': 'mean_temp',
        'V44': 'dhdt_fac',
        'V45': 'totwharvdep',
        'V46': 'stor_wat',
        'V47': 'pan_init_date',
        'V48': 'flowr_date',
        'V49': 'tot_irr_sup'
        }

var_units={'V1' : 'year',
           'V2' : 'degrees_north',
           'V3' : 'degrees_east',
           'V4' : 'days',
           'V5' : '1',
           'V6' : 'cm/cm^3',
           'V7' : 'm^2/m^2',
           'V8' : 'kg/ha',
           'V9' : 'kg/ha',
           'V10': '1',
           'V11': '1',
           'V12': 'cm',
           'V13': 'MJ/m^2',
           'V14': 'cm',
           'V15': 'cm',
           'V16': 'cm',
           'V17': 'cm',
           'V18': '1',
           'V19': 'cm',
           'V20': 'cm',
           'V21': 'cm',
           'V22': 'cm',
           'V23': 'cm',
           'V24': 'cm',
           'V25': 'cm',
           'V26': 'cm',
           'V27': 'cm',
           'V28': 'cm',
           'V29': 'm^2/m^2',
           'V30': 'cm',
           'V31': 'cm',
           'V32': 'cm/cm^2',
           'V33': 'cm/cm^2',
           'V34': 'cm',
           'V35': 'cm',
           'V36': 'MJ/m^2',
           'V37': 'days',
           'V38': 'kPa',
           'V39': 'MJ/m^2',
           'V40': '%',
           'V41': '%',
           'V42': '%',
           'V43': 'celcius',
           'V44': '1',
           'V45': 'cm',
           'V46': 'cm',
           'V47': 'day',
           'V48': 'day',
           'V49': 'cm'
           }

def nextPath(path_pattern):
    """
    Finds the next free path in an sequentially named list of files

    e.g. path_pattern = 'file-%s.txt':

    file-1.txt
    file-2.txt
    file-3.txt

    Runs in log(n) time where n is the number of existing files in sequence
    """
    i = 1

    # First do an exponential search
    while os.path.exists(path_pattern % i):
        i = i * 2

    # Result lies somewhere in the interval (i/2..i]
    # We call this interval (a..b] and narrow it down until a + 1 = b
    a, b = (i // 2, i)
    while a + 1 < b:
        c = (a + b) // 2 # interval midpoint
        a, b = (c, b) if os.path.exists(path_pattern % c) else (a, c)

    return path_pattern % b


def readargs():
    '''
    Read input arguments if run as separate program
    '''

    parser = argparse.ArgumentParser(description=(
        'Combine the raw data ouytputs for a particular climate scheme into'
        ' a single NetCDF file.'
        ))

    parser.add_argument('--dir', '-d',
                        type=str,
                        help='Path to directory for climate scheme',
                        default='.')

    parser.add_argument('--out', '-o',
                        type=str,
                        help='Directory for output file',
                        default='.')

    parser.add_argument('--proc', '-p',
                        type=int,
                        help='Number of parallel processes used for data reading and combination',
                        default=1)

    args = parser.parse_args()

    if not isinstance(args.dir,str):
        raise errlib.ArgumentsError("Data running directory is not a string!\n")

    if not isinstance(args.out,str):
        raise errlib.ArgumentsError("Output filename is not a string!\n")

    if not isinstance(args.proc,int):
        raise errlib.ArgumentsError("Number of parallel processes is not an integer!\n")

    if args.dir=='.':
        ascdir=os.getcwd()
    else:
        ascdir=args.dir

    if ascdir[-1]=="/":
        simval=ascdir.split('/')[-5:-1]
    else:
        simval=ascdir.split('/')[-4:]
        ascdir=ascdir+"/"
        
    print (simval)

    if not os.path.exists(ascdir):
        raise errlib.ArgumentsError('Path to data files does not exist: '
                             + ascdir+'\n')

    contents=[i for i in os.listdir(ascdir) if os.path.isdir(os.path.join(ascdir,i))]
    if not contents==[str(i) for i in range(1980,2100)]:
        raise errlib.ArgumentsError('Data file directory expected to contain 120 numbered folders\n'+
                             'numbered from 1980 to 2099 inclusive, but these folders were\n'+
                             'not found. Check that the directory argument is correct.\n'+
                             'Directory checked was '+ascdir+'\n')

    if not os.path.exists(args.out):
        	raise errlib.ArgumentsError('Directory to write netCDF file to'
                             + ' does not exist\n')

    if args.out and not os.path.isdir(args.out):
        	raise errlib.ArgumentsError('Directory to write netCDF file to'
                             + ' does not exist\n')

    try:
        os.makedirs(os.path.join(args.out,simval[0]))
    except FileExistsError:
        # directory already exists
        pass

    outfil=os.path.join(args.out,"ind_rcp",simval[0],"_".join(simval[1:])+".nc")

    if args.proc > 40:
        raise errlib.ArgumentsError("Too many processes requested. Maximum of 40\n")

    procs=args.proc

    retdata=[ascdir,simval,procs,outfil]

    return retdata

def getyrs(locdir):

    yrs=[]
    for fol in os.walk(locdir):
        if len(fol[2]) >= 120:
            yr=fol[0].split('/')[-1]
            if not yr=='':
                yrs.append(yr)

    return yrs

def readascii(path,dimvals):

    try:
        df=pd.read_csv(path,sep=' ')
    except:
        raise errlib.FileError("Error reading file at "+path+"\n")

    filenm = os.path.split(path)[1]

    n=df['V2'].max()
    s=df['V2'].min()
    e=df['V3'].max()
    w=df['V3'].min()

    if all(x == df['V1'][1] for x in df['V1']):
        time = iris.coords.DimCoord(df['V1'][1],standard_name="time",long_name="Time",var_name="time",units="year")
    else:
        print ("Error in data file "+filenm+".\n")
        print ("Multiple years read within same file")

    prodlev = iris.coords.DimCoord(float(filenm.split('_')[-3]),long_name="production level",var_name="prod_lev",units=1)
    irr_lev = iris.coords.DimCoord(float(filenm.split('_')[-2]),long_name="irrigation level",var_name="irr_lev",units=1)
    #country_dim = iris.coords.DimCoord(float(dimvals[0]),long_name="country",var_name="country",units=1)
    crop_dim = iris.coords.DimCoord(float(dimvals[1]),long_name="crop",var_name="crop",units=1)
    model_dim = iris.coords.DimCoord(float(dimvals[2]),long_name="climate model",var_name="model",units=1)
    rcp_dim = iris.coords.DimCoord(float(dimvals[3]),long_name="rep. conc. pathway",var_name="rcp",units=1)

    grid=np.zeros((1,int(((n-s)*2)+1),int(((e-w)*2)+1),1,1,1,1,1))
    grid.fill(-99)

    latitude  = iris.coords.DimCoord(np.linspace(s, n, int((n-s)*2)+1), standard_name='latitude',  units='degrees_north', long_name='Latitude',  var_name='lat')
    longitude = iris.coords.DimCoord(np.linspace(w, e, int((e-w)*2)+1), standard_name='longitude', units='degrees_east', long_name='Longitude', var_name='lon')

    cube_templ=iris.cube.Cube(grid, dim_coords_and_dims=[(time,0),(latitude,1),(longitude,2),(prodlev,3),(irr_lev,4),(rcp_dim,5),(model_dim,6),(crop_dim,7)])

    cubelist=iris.cube.CubeList([])
    for col in df:
        num=int(col[1:])
        if num > 3:
            cube_layer=cube_templ.copy()
            for index, row in df.iterrows():
                lat=row['V2']
                lon=row['V3']
                a=np.where(cube_layer.coord('latitude').points==float(lat))
                b=np.where(cube_layer.coord('longitude').points==float(lon))
                cube_layer.data[0,a[0][0],b[0][0],0,0,0,0,0]=row[col]

            cube_layer.data=np.ma.masked_equal(cube_layer.data,-99.)
            cube_layer.rename(var_nm[col])
            cube_layer.data.fill_value=-99.0

            cubelist.append(cube_layer)

    return cubelist

def fullyr(data):

    valnames=data[1][0]
    ascdir = data[1][1]
    dimvals = data[1][2]
    yr=data[0]

    cubelst=iris.cube.CubeList([])

    for prod in prod_lst:
        for irr in irr_lst:
            filenm=valnames[1]+"_"+valnames[0]+"_amma_"+valnames[2]+"_"
            filenm=filenm+valnames[3]+"_Fut_"+yr+"_"+prod+"_"+irr+"_1.out"
            path=ascdir+yr+"/"+filenm

            cubelst+=readascii(path, dimvals)

    return cubelst.concatenate()

def multiprocess_rcp (indata):

    [yrs,ascdir,valnames,procs,dimvals]=indata

    bigcubelist=iris.cube.CubeList([])

    intermediate_cubelist=iris.cube.CubeList([])

    locproc=min(len(yrs),procs)

    try:
        pool=Pool(processes=locproc)
    except:
        errlib.ArgumentsError("Setting processor pool failed\nprocs = {}\nlen(yrs) = {}\nlocproc= {}\n".format(procs,len(yrs),locproc))

    args=[valnames,ascdir,dimvals]

    itterable = [[yr, args] for yr in yrs]

    list_of_chunks=np.array_split(itterable,len(itterable)/locproc)

    start=time.time()

    for chunk in list_of_chunks:
        intermediate_cubelist+=pool.map(fullyr,chunk)

    for cubelst in intermediate_cubelist:
        bigcubelist+=cubelst

    end=time.time()

    outcubelst=bigcubelist.concatenate()

    print ('time to combine ascii to a single nc:',int(end-start))

    return outcubelst

def singleprocess_rcp (indata):

    [yrs,ascdir,valnames,procs,dimvals]=indata

    bigcubelist=iris.cube.CubeList([])

    start=time.time()

    args=[valnames,ascdir,dimvals]

    itterable = [[yr, args] for yr in yrs]

    for data in itterable:
        bigcubelist+=fullyr(data)

    end=time.time()

    outcubelst=bigcubelist.concatenate()

    print ('time to combine ascii to a single nc:',int(end-start))

    return outcubelst

def outcube(cube, fname):

    if (os.path.exists(fname)):
        outfile=nextPath(fname[:-3]+'_%s.nc')
    else:
        outfile=fname

    iris.fileformats.netcdf.save(cube, outfile, zlib=True)

def main():

    [ascdir,valnames,NBR_PROCESSES,outfil]=readargs()

    yrs=getyrs(ascdir)

    dimvals=[0,0,0,0]

    indata=[yrs,ascdir,valnames,NBR_PROCESSES,dimvals]

    if NBR_PROCESSES>1:

        rcp_cube=multiprocess_rcp(indata)

    else:

        rcp_cube=singleprocess_rcp(indata)

    outcube(rcp_cube, outfil)

if __name__=="__main__":
    main()
